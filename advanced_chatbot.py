import os
import streamlit as st
from streamlit_chat import message
#  Ø£Ø¯Ø§Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø¨Ø­Ø«ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
from langchain.indexes import VectorstoreIndexCreator

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø³Ù„Ø³Ù„Ø© Ù…Ø­Ø§Ø¯Ø«Ø© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªØŒ ØªÙ…ÙƒÙ† Ù…Ù† Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø¯Ø±Ø¯Ø´Ø© ÙŠÙ…ÙƒÙ†Ù‡ ØªØ°ÙƒØ± Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø§Ø¨Ù‚
from langchain.chains import ConversationalRetrievalChain
# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø£Ø¯Ø§Ø© Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù†ØµÙŠØ©
from langchain_community.document_loaders import TextLoader

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø£Ø¯Ø§Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…ØªØ¬Ù‡ÙŠØ© Ù„Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenAI
from langchain_community.embeddings import OpenAIEmbeddings
# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù†Ù…ÙˆØ°Ø¬ Ø¯Ø±Ø¯Ø´Ø© Ù…Ù† OpenAI Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ©
from langchain_community.chat_models import ChatOpenAI

import warnings
openkey = "skproj-_AvVeTAKicteVqwXY86Rnv5O1nzLQdLg7OMC_UjhUaT8dsVP36ew8Y4PIdtkO0oAmBKLan5kZDT3BlbkFJducz93BQgy9wlM6DT7ygECE8X7rbPHMRCgjq3sxk5JdpHtcH3DOHWL-U659igpAmidtYTmMtQA"
warnings.filterwarnings("ignore")

st.title("Harmony | Ù‡Ø§Ø±Ù…ÙˆÙ†ÙŠ")
st.image("untitled.png")
st.markdown("""
**Ø§Ù†Ø§ Ù†Ù…ÙˆØ°Ø¬ Ø°ÙƒØ§Ø¡ ØµÙ†Ø§Ø¹ÙŠ Ø§Ù‚Ø¯Ù… Ù„Ùƒ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:**
- ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„ØªÙƒ Ø¨Ø®ØµÙˆØµ Ù…Ù†ØªØ¬Ø§Øª Ù‡Ø§Ø±ÙˆÙ…ÙŠ.
- Ø§Ø¬ÙŠØ¨ Ø¹Ù„Ù‰ ÙƒØ§Ù…Ù„ Ø§Ø³ØªÙØ³Ø§Ø±Ø§ØªÙƒ Ø¨Ø®ØµÙˆØµ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù….
- Ø­Ø¯Ø¯ Ù„ÙŠ Ù†ÙˆØ¹ Ø´Ø¹Ø±Ùƒ ÙˆÙ…Ø´ÙƒÙ„ØªÙ‡ ÙˆØ§Ø­Ø¯Ø¯ Ù„Ùƒ Ø§Ù„Ù…Ù†ØªØ¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨.
- ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ù…ØªØ§Ø¨Ø¹Ø© Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.
""")
st.divider()

data_file = r"data.txt"
request_container = st.container()
response_container = st.container()

loader = TextLoader(data_file)
loader.load()

from langchain.embeddings.openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(openai_api_key=openkey)


index = VectorstoreIndexCreator(embedding=embeddings).from_loaders([loader])

chain = ConversationalRetrievalChain.from_llm(llm=ChatOpenAI(model="gpt-3.5-turbo"),
                                              retriever=index.vectorstore.as_retriever(search_kwargs={"k": 1}))

if 'history' not in st.session_state:  # Ù‡Ù†Ø§ Ù‚Ø§Ø¦Ù…Ø© ÙÙŠÙ‡Ø§ Ù‚ÙˆØ§Ù…ÙŠØ³ ÙƒÙ„ Ù‚Ø§Ù…ÙˆØ³ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ø³Ø¤Ø§Ù„ ÙŠÙ…Ø«Ù„ Ø§Ù„Ù…ÙØªØ§Ø­ ÙˆØ§Ù„Ø¬ÙˆØ§Ø¨ ÙŠÙ…Ø«Ù„ Ø§Ù„Ù‚ÙŠÙ…Ø©
    st.session_state['history'] = []

if 'generated' not in st.session_state:  # Ù‡Ù†Ø§ Ù†Ø®Ø²Ù† ÙÙ‚Ø· Ù‚ÙŠÙ… Ø§Ù„Ø§Ø¬ÙˆØ¨Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©
    st.session_state['generated'] = ["Ù…Ø±Ø­Ø¨Ø§..Ø§Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ø³Ø£Ù„ ÙƒÙ„ Ù…Ø§ ØªØ­Ø¨ Ø¨Ø®ØµÙˆØµ Ù…Ù†ØªØ¬Ø§Øª Ù‡Ø§Ø±Ù…ÙˆÙ†ÙŠ"]

if 'past' not in st.session_state:  # Ù‡Ù†Ø§ Ù†Ø®Ø²Ù† ÙÙ‚Ø· Ø§Ù„Ø§Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø©
    st.session_state['past'] = ["Hey ! ğŸ‘‹"]


def conversational_chat(prompt):
    result = chain({"question": prompt, "chat_history": st.session_state['history']})
    st.session_state['history'].append((prompt, result["answer"]))
    return result["answer"]


with request_container:
    with st.form(key='xyz_form', clear_on_submit=True):
        user_input = st.text_input("Prompt:", placeholder="Message HarmonyAI...", key='input')
        submit_button = st.form_submit_button(label='Send')

    if submit_button and user_input:
        output = conversational_chat(user_input)

        st.session_state['past'].append(user_input)
        st.session_state['generated'].append(output)

if st.session_state['generated']:
    with response_container:
        for i in range(len(st.session_state['generated'])):
            message(st.session_state["past"][i], is_user=True, key=str(i) + '_user', avatar_style="adventurer", seed=13)
            message(st.session_state["generated"][i], key=str(i), avatar_style="bottts", seed=2)